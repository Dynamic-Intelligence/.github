# Dynamic Intelligence

<img src="https://avatars.githubusercontent.com/u/252190853?s=200&v=4" align="right" width="120">

**Harvard & MIT team building the data bedrock for VLA models that power humanoid robots and autonomous physical agents.**

## ğŸ¯ What We Do

We create high-quality datasets for training Vision-Language-Action (VLA) foundation models â€” the AI systems that enable robots to see, understand, and act in the physical world.

## ğŸ”¬ Our Focus

- **Egocentric Motion Data** â€” First-person hand pose trajectories for manipulation tasks
- **6-DoF Pose Estimation** â€” Full position and orientation tracking at 30 FPS
- **LeRobot Compatible** â€” Ready for state-of-the-art imitation learning

## ğŸ“¦ Datasets

| Dataset | Description |
|---------|-------------|
| [Egocentric Hand Pose Dataset](https://huggingface.co/datasets/DynamicIntelligence/humanoid-robots-training-dataset) | 97 episodes, 10 manipulation tasks, ~28K frames |

## ğŸ”— Links

- ğŸŒ [Website](https://dynamicintelligence.company)
- ğŸ¤— [HuggingFace](https://huggingface.co/DynamicIntelligence)
- ğŸ“§ [Contact](mailto:shayan@dynamicintelligence.company)

---

*Building the future of embodied AI, one dataset at a time.*
